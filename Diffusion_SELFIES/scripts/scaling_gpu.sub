# Scaling Law Experiment Runner (SELFIES)
# ==========================================
# SUBMISSION COMMANDS (run from /home/htang228):
#   mkdir -p logs
#   # Create tarball (excludes old results):
#   cd /mnt/htc-cephfs/fuse/root/staging/htang228/diffusion_model/_0106_2026_diffusion && \
#       tar --exclude='results_*' -czf /home/htang228/Diffusion_SELFIES.tar.gz Diffusion_SELFIES/
#   # Submit job:
#   condor_submit /mnt/htc-cephfs/fuse/root/staging/htang228/diffusion_model/_0106_2026_diffusion/Diffusion_SELFIES/scripts/scaling_gpu.sub
#   # Or with different model size:
#   condor_submit -append "MODEL_SIZE=large" <path_to_this_file>
# ==========================================

# === EASY CONFIG ===
BASE_PATH = /mnt/htc-cephfs/fuse/root/staging/htang228/diffusion_model/_0106_2026_diffusion
METHOD = Diffusion_SELFIES

# HTCondor settings
universe   = vanilla
initialdir = /home/htang228

# Model size: small, medium, large, xl
MODEL_SIZE = medium

# Logs in /home/ (HTC requirement)
log    = logs/sel_$(MODEL_SIZE)_$(Cluster)_$(Process).log
output = logs/sel_$(MODEL_SIZE)_$(Cluster)_$(Process).out
error  = logs/sel_$(MODEL_SIZE)_$(Cluster)_$(Process).err

# Resources
request_gpus   = 1
+WantGPULab    = true
+GPUJobLength  = "long"
request_cpus   = 16
request_memory = 256GB
request_disk   = 50GB

# File transfer (enables more HTC nodes)
should_transfer_files   = YES
when_to_transfer_output = ON_EXIT

# Executable is inside tarball - use bash wrapper
executable = /bin/bash
transfer_input_files    = $(METHOD).tar.gz
transfer_output_files   = results_$(MODEL_SIZE).tar.gz
transfer_output_remaps  = "results_$(MODEL_SIZE).tar.gz=$(BASE_PATH)/$(METHOD)/results_$(MODEL_SIZE)_$(Cluster).tar.gz"

# Run the script from inside the tarball
arguments = "-c ""tar -xzf $(METHOD).tar.gz && cd $(METHOD) && bash scripts/run_scaling_condor.sh $(MODEL_SIZE)"""

queue 1
